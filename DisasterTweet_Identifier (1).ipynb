{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DisasterTweet_Identifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fS4Yd5jYWwz",
        "colab_type": "text"
      },
      "source": [
        "**Authors**:\n",
        "* *Joshua Frenchwood*\n",
        "* *Sean Schuepbach*\n",
        "* *Sai Sandeep Kumar*\n",
        "* *Manikanta Addanki*\n",
        "\n",
        "**Date**: *May 5th, 2020*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNRYvWa4Cs77",
        "colab_type": "text"
      },
      "source": [
        "*** *Note*: This uses Google's server to run the code remotely on a large supercomputer, and is therefore much faster than trying to run this code on the average local computer. ***\n",
        "\n",
        "**For improved execution speed**: \n",
        "* Try using the GPU or TPU provided by Google Colab to run the code (not necesarry)...\n",
        " * Edit -> Notebook Settings -> Hardware Accelerator -> GPU or TPU\n",
        "\n",
        "**Github Link**: https://github.com/langadudeabu/Tweet-Classifier\n",
        "\n",
        "**To Run the Code**: Press Ctrl+F9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD2pXBdb-gw-",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuU3eP6l5jbR",
        "colab_type": "text"
      },
      "source": [
        "# Imports and Globals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbYIwUdbaV49",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "846f48c9-a994-47bc-ee02-fc6b22286fdc"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize, TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import random\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from nltk.classify import ClassifierI\n",
        "from statistics import mode, mean"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCP1E8ya_mYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 8000 # number of features in algorithms ( top n most common words )\n",
        "percent_train = 80\n",
        "\n",
        "ORIG = 1 # 1 when you want to include it in the active dataset\n",
        "CHI = 1  # 1 when you want to include it in the active dataset\n",
        "HOU = 1  # 1 when you want to include it in the active dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJFf4cSr5hfW",
        "colab_type": "text"
      },
      "source": [
        "# Functions and Descriptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSXCsgqsZyRz",
        "colab_type": "text"
      },
      "source": [
        "Class `VoteClassifier`:\n",
        "* Inherits from ClassifierI (has compatability with `nltk.classify.accuracy()` function, the same function used to determine accuracy for each individual classifier).\n",
        "* `classify()` member function uses voting system to make a prediction\n",
        "* `confidence()` member function returns the fraction of majority votes / total votes, and can be either 60%, 80%, or 100%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImuZzKX02YiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VoteClassifier(ClassifierI): #inherits from ClassifierI\n",
        "  def __init__(self, *classifiers): #pass in a list of different Classifiers\n",
        "    self._classifiers = classifiers\n",
        "    self._name_list = [ 'Naive_Bayes_Classifier',\n",
        "              'MultinomialNB_classifier',\n",
        "              'BernoulliNB_classifier',\n",
        "              'LogisticRegression_classifier',\n",
        "              'SGDClassifier_classifier' ]\n",
        "\n",
        "  def classify(self, tweetVector, p=0):\n",
        "    votes = []\n",
        "    for i in range(len(self._classifiers)):\n",
        "      c = self._classifiers[i]\n",
        "      v = c.classify(tweetVector) # from nltk.classify\n",
        "      if p :\n",
        "        print(self._name_list[i],':', v)\n",
        "      votes.append(v)\n",
        "    \n",
        "    self.classification = votes\n",
        "    bestPick = mode(votes) # from statistics.mode\n",
        "    return bestPick\n",
        "    \n",
        "  def confidence(self, tweetVector):\n",
        "    bestPick = self.classify(tweetVector, p=1)\n",
        "    choice_votes = self.classification.count(bestPick)\n",
        "    conf = choice_votes / len(self.classification)\n",
        "    return conf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFn6UTcObTar",
        "colab_type": "text"
      },
      "source": [
        "Function `removeApostrophe()`:\n",
        "* **Inputs**: tweet that needs apostrophe removed as a string\n",
        "* **Globals**: None.\n",
        "* **Outputs**: tweet without apostrophe as string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYiiHQBM2aFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def removeApostrophe(tweet):\n",
        "  return tweet.replace('\\'', '')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxYsbuw4ceyR",
        "colab_type": "text"
      },
      "source": [
        "Function `cleanTweet`:\n",
        "* **Description**: This function removes irrelevent words from the tweet such as \"of\" and \"the\", along with symbols, hyperlinks, emojis, numbers, punctuation, and apostrophes.\n",
        "* **Inputs**: Unmodified tweet as string.\n",
        "* **Globals**: None.\n",
        "* **Outputs**: Cleaned tweet as string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH9mjPdP2dZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleanTweet(tweet):\n",
        "  t = TweetTokenizer()\n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  more = [';', '/','.',':','!','of','?','-','...','&','\"\"','',\n",
        "          \"'s\",\"'m\",'\\'','\\'s','\\'re','\\'t','[',']','(',')',\n",
        "          'û','ûª', 'ûªt', 'ûªs', 'ûªve', 'ûªre', 'ûªm']\n",
        "  for letts in more:\n",
        "    stop_words.add(letts)\n",
        "\n",
        "  def isSymbol(word):\n",
        "    syms = 0\n",
        "    for i in range(len(word)):\n",
        "      if(not word[i].isalpha()):\n",
        "        syms = syms + 1\n",
        "    if(syms >= len(word)/2): return True\n",
        "    else: return False\n",
        "\n",
        "  words = t.tokenize(tweet)\n",
        "  resultwords  = [w.lower() for w in words \n",
        "                  if w not in stop_words \n",
        "                  and not w.startswith('http') \n",
        "                  and not w.startswith('@') \n",
        "                  and not w.isnumeric() \n",
        "                  and not isSymbol(w) \n",
        "                  and not len(w) <= 2\n",
        "                  ]\n",
        "                  #and not len(w) <= 2\n",
        "  tweet = ' '.join(resultwords)\n",
        "  tweet = removeApostrophe(tweet)\n",
        "  return tweet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-GSRF5KdN4i",
        "colab_type": "text"
      },
      "source": [
        "Function `cleanTweets()`:\n",
        "* **Description**: This function goes through a list of tweets, and cleans them individually, by passing them to the function `cleanTweet()`\n",
        "* **Inputs**: list of tweets to be cleaned (string form)\n",
        "* **Globals**: None.\n",
        "* **Outputs**: None. Just modifies the input parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ_nhfTJ2fE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleanTweets(tweets):  \n",
        "  for i in range(len(tweets)):\n",
        "    tweets[i] = cleanTweet(tweets[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4CxQuGAeCzG",
        "colab_type": "text"
      },
      "source": [
        "Function `y_convert()`:\n",
        "* **Description**: This function translates the dataset's y-values (list of actual classifications) into either *disaster* or *non-disaster*.\n",
        "* **Inputs**: `y_train` is a list of actual classification values from either ORIG, CHI, or HOU. The Original dataset has integer values (0 or 1), and the Chicago and Houston datasets have string values (\"relevent\" or \"non-relevent\")\n",
        "* **Globals**: None.\n",
        "* **Outputs**: `y_train2` is a list of string values (\"disaster\" or \"non-disaster\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyPAXL292g23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def y_convert(y_train):\n",
        "  y_train2 = []\n",
        "  for i in range(len(y_train)) :\n",
        "    if (y_train[i] == 1) or (y_train[i] == 'relevent'):\n",
        "      y_train2.append('disaster')\n",
        "    else:\n",
        "      y_train2.append('non-disaster')\n",
        "  return y_train2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nQhrEMIf6-9",
        "colab_type": "text"
      },
      "source": [
        "Function `read_in_tweets()`:\n",
        "* **Description**: This function reads in data from the csv files located in our github repository. The `cleanTweets()` function cleans the tweets, and the `y_convert()` function translates the classifications to either \"disaster\" or \"non-disaster\".\n",
        "* **Inputs**: None. The urls are hardcoded to our github repository.\n",
        "* **Outputs**: Several Series (pandas Series) containing the CLEANED tweet data and the TRANSLATED classification values for each of the 3 datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0-0WCmR2kqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_in_tweets():\n",
        "  url = 'https://raw.githubusercontent.com/langadudeabu/MachineLearningData/master/train.csv'\n",
        "  dataOrig = pd.read_csv(url) # Returns a pandas.DataFrame\n",
        "  url = 'https://raw.githubusercontent.com/langadudeabu/nlp-disaster-analysis/master/dataset/recent_tweets_test/chicago_tweets-labeled.csv'\n",
        "  dataChi = pd.read_csv(url) # Returns a pandas.DataFrame\n",
        "  url = 'https://raw.githubusercontent.com/glrn/nlp-disaster-analysis/master/dataset/recent_tweets_test/houston_tweets-labeled.csv'\n",
        "  dataHou = pd.read_csv(url) # Returns a pandas.DataFrame\n",
        "\n",
        "  train_tweets = dataOrig.pop('text').str.lower() # Returns a pandas.Series\n",
        "  y_train = dataOrig.pop('target') # Returns a pandas.Series\n",
        "\n",
        "  chi_tweets = dataChi.pop('text').str.lower() # Returns a pandas.Series\n",
        "  chi_results = dataChi.pop('choose_one').str.lower() # Returns a pandas.Series\n",
        "\n",
        "  hou_tweets = dataHou.pop('text').str.lower() # Returns a pandas.Series\n",
        "  hou_results = dataHou.pop('choose_one').str.lower() # Returns a pandas.Series\n",
        "\n",
        "  cleanTweets(train_tweets)\n",
        "  cleanTweets(chi_tweets)\n",
        "  cleanTweets(hou_tweets)\n",
        "\n",
        "  y_trainOrig = y_convert(y_train)\n",
        "  y_trainChi = y_convert(chi_results)\n",
        "  y_trainHou = y_convert(hou_results)\n",
        "\n",
        "  return train_tweets, y_trainOrig, chi_tweets, y_trainChi, hou_tweets, y_trainHou"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ILw2FFgh_Db",
        "colab_type": "text"
      },
      "source": [
        "Function `combineDatasets()`:\n",
        "* **Description**: This function appends specified datasets together into one cohesive dataset. The tweets and results are appended to separate lists, but indexed to the same length.\n",
        "* **Inputs**: `orig`, `chi`, and `hou` are boolean values. A value of 1 (True) means the particular dataset will be included in the overall training dataset.\n",
        "* **Globals**: `train_tweets`, `y_trainOrig`, `chi_tweets`, `y_trainChi`, `hou_tweets`, and `y_trainHou` are all global *Pandas Series* variables.\n",
        "* **Outputs**: `data` is a list of tweets(strings), and `results` is a list of corresponding classification values (strings - \"disaster\" or \"non-disaster\") "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dykUpTfF2tVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def combineDatasets(orig=1, chi=0, hou=0):\n",
        "  data = []\n",
        "  results = []\n",
        "  if orig :\n",
        "    for i in range(len(train_tweets)):\n",
        "      data.append(train_tweets[i])\n",
        "      results.append(y_trainOrig[i])\n",
        "  if chi :\n",
        "    for i in range(len(chi_tweets)):\n",
        "      data.append(chi_tweets[i])\n",
        "      results.append(y_trainChi[i])\n",
        "  if hou :\n",
        "    for i in range(len(hou_tweets)):\n",
        "      data.append(hou_tweets[i])\n",
        "      results.append(y_trainHou[i])\n",
        "\n",
        "  return data, results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kOwkatrkt4y",
        "colab_type": "text"
      },
      "source": [
        "Function `apply_BagOfWords()`:\n",
        "* **Description**: This function applies the Bag Of Words model, creating frequency distribution of all words in the collection of cleaned tweets. Also creates a list of tuples to hold the tweets (in word tokenized form) next to their corresponding classification values.\n",
        "* **Globals**: `train_data` and `train_results` are lists of tweets and results from our overall combined dataset.\n",
        "* **Outputs**: `docs` is a list of tuples, each holding the tokenized tweet alongside the corresponding classification. `wordFrequencies` is a list of dictionary values: ( word : total # of occurences )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xIxsfl32u0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_BagOfWords(train_data, train_results):\n",
        "  t = TweetTokenizer()\n",
        "  tokenizedData = []\n",
        "  for i in range(len(train_data)):\n",
        "    tweet = train_data[i]\n",
        "    classification = train_results[i]\n",
        "    tokenizedTweet = list(t.tokenize(tweet)) #tokenize one tweet at a time\n",
        "    \n",
        "    tup = (tokenizedTweet, classification)\n",
        "    tokenizedData.append(tup)\n",
        "  \n",
        "  random.shuffle(tokenizedData)\n",
        "\n",
        "  all_words = [ w for tweet in train_data for w in t.tokenize(tweet) ] \n",
        "  wordFrequencies = nltk.FreqDist(all_words)\n",
        "\n",
        "  return tokenizedData, wordFrequencies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDtpmv7hriwI",
        "colab_type": "text"
      },
      "source": [
        "Function `vectorizeTweet()`:\n",
        "* **Description**:  This function fills in the vectorized representation of a single tweet with Boolean values (True if contains feature, False if not).\n",
        "* **Inputs**: `tokenizedTweet` is a word-tokenized version of a single tweet\n",
        "* **Globals**: `wordFrequencies` is a list of BagOfWords dictionary values: ( word : total # of occurences )\n",
        "* **Outputs**: `features` is a list of dictionary values: ( feature_word : Boolean ), where the Boolean is True if that specific tweet contains the feature, and False if not. This is the *vectorized form of the tweet* that will be fed into the ML algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laCLIL8X2wTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorizeTweet(tokenizedTweet, word_features):\n",
        "    words = set(tokenizedTweet)\n",
        "    features = {}\n",
        "    for w in word_features :\n",
        "      features[w] = (w in words) # the Value of the dictionary is a BOOLEAN\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnNwJTXmvl37",
        "colab_type": "text"
      },
      "source": [
        "Function `vectorizeTweets()`:\n",
        "* **Description**: This function takes the top n values from the top of the wordFrequency list (top n most common words) and makes them into a list of features. Also, this function creates a list of vectorized tweets, individually vectorizing them with the `vectorizeTweet()` function, and links them to their corresponding classification by returning them in tuple format.\n",
        "* **Inputs**: None. Just references globals.\n",
        "* **Globals**: `n` is an integer that represents the desired length of the featureset (top *n* most common words). `tokenizedData` is list of tuples containing: ( word-tokenized tweet : classification ).\n",
        "* **Outputs**: `tweetVectorData` is a list of tuples, containing: ( vectorized tweet : classification ). This is the format that will be fed into the machine learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPWJdBBV2yOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorizeTweets():\n",
        "  featureList = list( wordFrequencies.keys() )[:n] # Top n most common words\n",
        "  tweetVectorData = [ ( vectorizeTweet(tokenizedTweet, featureList) , classification ) \n",
        "                 for (tokenizedTweet, classification) in tokenizedData]\n",
        "\n",
        "  return tweetVectorData, featureList"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M5urZfgwRJw",
        "colab_type": "text"
      },
      "source": [
        "Function `splitData()`:\n",
        "* **Description**: This function takes the Vectorized tweet data along with their corresponding classifications (tuple form), and splits them into two separate variables, one to be used for Training, and one to be used for calculating Accuracy.\n",
        "* **Inputs**: None. Just references globals.\n",
        "* **Globals**: `percent_train` is an integer representing the percentage of the total dataset that will be allocated for training purposes. `tweetVectorData` holds the vectorized form of each tweet along with the corresponding classifications (tuple form).\n",
        "* **Outputs**: `training_set` holds the vectorized training data along with the corresponding answers for the ML algorithms to train on. `testing_set` holds the vectorized testing data along with the corresponding answers, used for calculating Accuracy. `cap` is an index for reference if needed, to refer back to the main tweetVectorData (acts as a zero point of index)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmWSH4cb2zx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def splitData():\n",
        "  cap = int( len(train_data) * percent_train/100 )\n",
        "  training_set = tweetVectorData[:cap]\n",
        "  testing_set = tweetVectorData[cap:]\n",
        "  return training_set, testing_set, cap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afz9z0r84IMT",
        "colab_type": "text"
      },
      "source": [
        "Function `readFromKeyboard()`:\n",
        "* **Description**: This function reads sentence in from the keyboard (until *Enter* is pressed), cleans the \"tweet\", vectorizes the tweet, and returns the vectorized form of the entered tweet. *Note: only knows the tweet, does not know the classification...*\n",
        "* **Inputs**: The values input to the Keyboard.\n",
        "* **Globals**: `featureList` is required, so that the tweet can be vectorized along the elements in the list of working features.\n",
        "* **Outputs**: `tweet` is the raw tweet as entered in the keyboard. `clean_tweet` is the cleaned version of the tweet (only relevent words). `tweetVector` is the Vectorized version of the tweet (same length as *featureList*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuemYUbf5-0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readFromKeyboard():\n",
        "  t = TweetTokenizer()\n",
        "  tweet = input(\"Enter Tweet: \")\n",
        "  clean_tweet = cleanTweet(tweet)\n",
        "  print('Cleaned Tweet:', clean_tweet,'\\n')\n",
        "\n",
        "  tweetVector = vectorizeTweet(t.tokenize(clean_tweet), featureList)\n",
        "  return tweet, clean_tweet, tweetVector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ThmnXYE8sGF",
        "colab_type": "text"
      },
      "source": [
        "Function `enterTweet()`:\n",
        "* **Description**: This function allows the User to input their own \"tweet\" and makes a prediction of whether the input tweet was referring to a disaster or a non-disaster, using the `voted_classifier` (uses 5-classifier voting system).\n",
        "* **Inputs**: None. Tweet will eventually be entered in through the keyboard.\n",
        "* **Globals**:  `voted_classifier` (object from ClassifierI family) is a *VoteClassifier* object, which uses member functions `self.classify()` and `self.confidence()` to output the results.\n",
        "* **Outputs**: Prints the voting system's prediction, along with its confidence value. Also, prints the predictions of each individual classifier in the voting system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Leu1d36e5_uq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def enterTweet():\n",
        "  _, _, tweetVector = readFromKeyboard() \n",
        "  classification = voted_classifier.classify(tweetVector)\n",
        "  confidence = voted_classifier.confidence(tweetVector)*100\n",
        "\n",
        "  print(\"\\nClassification:__\", classification, \n",
        "        \"__\\tConfidence %:__\", confidence, '__')\n",
        "  print('----------------------------------------------------------\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYjYCfUv-cEs",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgwKpfRJ2_Cv",
        "colab_type": "text"
      },
      "source": [
        "# Load and Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR8_IqG02-oA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tweets, y_trainOrig, chi_tweets, y_trainChi, hou_tweets, y_trainHou = read_in_tweets()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ACgSxMQ3GHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, train_results = combineDatasets(orig=ORIG, chi=CHI, hou=HOU)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxnfz3cd3l9t",
        "colab_type": "text"
      },
      "source": [
        "# Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od7JIM6g3Mqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizedData, wordFrequencies = apply_BagOfWords(train_data, train_results) # does frequency distribution, shuffles data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SISIH6ni3seE",
        "colab_type": "text"
      },
      "source": [
        "# Create Features / Vectorize Tweets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp1vL2Bk3dT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweetVectorData, featureList = vectorizeTweets() # returns list of tuples: (vectorized tweet : classification), and the list of features\n",
        "training_set, testing_set, cap = splitData() # splits tweetVectorData into two sets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDe60EQC375x",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t11z1qUy3-Lh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = nltk.NaiveBayesClassifier.train(training_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSymGAlL4IX7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fbee680d-6c82-4509-f94b-541d3ed92503"
      },
      "source": [
        "MultinomialNB_classifier = SklearnClassifier(MultinomialNB())\n",
        "MultinomialNB_classifier.train(training_set)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SklearnClassifier(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYg229CV4ItP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "303e338a-cf25-4a89-a728-fe1c565ded51"
      },
      "source": [
        "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
        "BernoulliNB_classifier.train(training_set)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SklearnClassifier(BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True))>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSSn-N2l4JCL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "3bf6d3e5-4881-470c-d305-fe1f00d536ea"
      },
      "source": [
        "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
        "LogisticRegression_classifier.train(training_set)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SklearnClassifier(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False))>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-u0feVo4Jk1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "5b0cb3b4-58b6-413a-cc57-450c207cc625"
      },
      "source": [
        "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
        "SGDClassifier_classifier.train(training_set)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SklearnClassifier(SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
              "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
              "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
              "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
              "              validation_fraction=0.1, verbose=0, warm_start=False))>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cnue0fDi4KO5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "63dd7a91-675a-4b2a-ffbe-bcac07ad3c45"
      },
      "source": [
        "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
        "LinearSVC_classifier.train(training_set)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SklearnClassifier(LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "          verbose=0))>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7jc5NYY44zK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "voted_classifier = VoteClassifier(classifier,\n",
        "                                  MultinomialNB_classifier,\n",
        "                                  BernoulliNB_classifier,\n",
        "                                  LogisticRegression_classifier,\n",
        "                                  SGDClassifier_classifier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD_Udisu4ZXf",
        "colab_type": "text"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgg5dR3-4b0T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "f013b7b6-b746-49da-d05f-0313a704238f"
      },
      "source": [
        "classifier.show_most_informative_features(25)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most Informative Features\n",
            "               hiroshima = True           disast : non-di =     88.4 : 1.0\n",
            "                northern = True           disast : non-di =     86.8 : 1.0\n",
            "                malaysia = True           disast : non-di =     63.6 : 1.0\n",
            "                  debris = True           disast : non-di =     57.0 : 1.0\n",
            "                wildfire = True           disast : non-di =     47.1 : 1.0\n",
            "                   kills = True           disast : non-di =     47.1 : 1.0\n",
            "                  atomic = True           disast : non-di =     46.1 : 1.0\n",
            "                 bombing = True           disast : non-di =     40.7 : 1.0\n",
            "                 reunion = True           disast : non-di =     38.8 : 1.0\n",
            "                 typhoon = True           disast : non-di =     35.5 : 1.0\n",
            "                    amid = True           disast : non-di =     33.9 : 1.0\n",
            "                 suicide = True           disast : non-di =     32.6 : 1.0\n",
            "                  feared = True           disast : non-di =     32.2 : 1.0\n",
            "                   crews = True           disast : non-di =     32.2 : 1.0\n",
            "                   spill = True           disast : non-di =     30.2 : 1.0\n",
            "             anniversary = True           disast : non-di =     29.3 : 1.0\n",
            "                rescuers = True           disast : non-di =     29.3 : 1.0\n",
            "                   drown = True           non-di : disast =     29.1 : 1.0\n",
            "                soudelor = True           disast : non-di =     28.9 : 1.0\n",
            "                   saudi = True           disast : non-di =     28.3 : 1.0\n",
            "                 outrage = True           disast : non-di =     27.3 : 1.0\n",
            "                   mount = True           disast : non-di =     24.0 : 1.0\n",
            "                 houston = True           non-di : disast =     23.5 : 1.0\n",
            "                 muslims = True           disast : non-di =     20.7 : 1.0\n",
            "                  crisis = True           disast : non-di =     19.3 : 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgDJ00CB4ioN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy1 = (nltk.classify.accuracy(classifier, testing_set))*100\n",
        "accuracy2 = (nltk.classify.accuracy(MultinomialNB_classifier, testing_set))*100\n",
        "accuracy3 = (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100\n",
        "accuracy4 = (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100\n",
        "accuracy5 = (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100\n",
        "accuracy6 = (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100\n",
        "votedAccuracy = (nltk.classify.accuracy(voted_classifier, testing_set))*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nfFIJTy4yQD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "2cd70553-af51-45fd-c4ff-aa95d4317ed5"
      },
      "source": [
        "print(\"Naive Bayes Algo accuracy percent: \", accuracy1)\n",
        "print(\"MultinomialNB_classifier Algo accuracy percent: \", accuracy2)\n",
        "print(\"BernoulliNB_classifier Algo accuracy percent: \", accuracy3)\n",
        "print(\"LogisticRegression_classifier Algo accuracy percent: \", accuracy4)\n",
        "print(\"SGDClassifier_classifier Algo accuracy percent: \", accuracy5)\n",
        "print(\"LinearSVC_classifier Algo accuracy percent: \", accuracy6)\n",
        "print(\"voted_classifier accuracy percent:\", votedAccuracy)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes Algo accuracy percent:  83.93574297188755\n",
            "MultinomialNB_classifier Algo accuracy percent:  83.89112003569835\n",
            "BernoulliNB_classifier Algo accuracy percent:  83.80187416331995\n",
            "LogisticRegression_classifier Algo accuracy percent:  83.75725122713075\n",
            "SGDClassifier_classifier Algo accuracy percent:  81.25836680053547\n",
            "LinearSVC_classifier Algo accuracy percent:  81.07987505577867\n",
            "voted_classifier accuracy percent: 84.24810352521196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbM4mseZSLq2",
        "colab_type": "text"
      },
      "source": [
        "# Try it Yourself"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r858rneKAUjq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "9ca5b4a6-5db8-4a69-ffab-3f8efe80cded"
      },
      "source": [
        "enterTweet()"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter Tweet: Texas man kills 30 in school shooting\n",
            "Cleaned Tweet: texas man kills school shooting \n",
            "\n",
            "classifier : disaster\n",
            "MultinomialNB_classifier : disaster\n",
            "BernoulliNB_classifier : disaster\n",
            "LogisticRegression_classifier : disaster\n",
            "SGDClassifier_classifier : disaster\n",
            "\n",
            "Classification:__ disaster __\tConfidence %:__ 100.0 __\n",
            "----------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}